---
name: powerbi-dax-performance
description: Guide to optimize DAX query performance using the Power BI Modeling MCP Server. Use this skill when asked to optimize DAX queries, troubleshoot slow queries, analyze DAX performance, tune query execution, investigate SE/FE bottlenecks, analyze xmSQL, or reduce query execution time. Do NOT use for creating or editing measures (use powerbi-semantic-model), TMDL syntax (use powerbi-tmdl), report layout/visual authoring (use powerbi-pbir), or workspace administration (use fabric-cli).
---

# DAX Performance Optimization Skill

This skill provides a structured workflow for optimizing DAX query performance using the `powerbi-modeling-mcp` server tools. There are three tiers of optimization, each with different levels of autonomy:

1. **Measure & UDF Optimization (auto-apply)**: Rewrite measure and user-defined function definitions to be more efficient. These changes preserve query semantics — the agent can experiment freely without user approval.
2. **Query Structure Changes (ask first)**: Modify the EVALUATE clause, grouping columns, or time grain. These change what the query returns. The agent must present recommendations and get explicit user permission before applying.
3. **Model Changes (ask first, high caution)**: Modify the semantic model itself (relationships, columns, aggregation tables, data types). These can break downstream reports. The agent must explain trade-offs, suggest working on a copy of the model, and note that source-level changes (Lakehouse, Warehouse, Power Query) may be needed. Implementation may require other skills (`powerbi-semantic-model`, `fabric-cli`) and must follow the user's CI/CD process.

Refer to `references/dax-performance-reference.md` for engine architecture, trace diagnostics, and the complete catalog of DAX, query structure, model, and data layout optimization patterns.

## IMPORTANT

- Requires the `powerbi-modeling-mcp` MCP Server. All operations use its tools.
- You must be connected to a semantic model before starting. Use `connection_operations` to connect if not already connected.
- **Tier 1 (Measures/UDFs)**: Optimize freely. Keep the EVALUATE / SUMMARIZECOLUMNS grouping identical between baseline and optimized queries. Since most queries are generated by visuals, users only have control over the measure and function definitions.
- **Tier 2 (Query Structure)**: Only after presenting your recommendation and receiving user approval. Explain what changes and how it affects the output.
- **Tier 3 (Model Changes)**: Only after detailed discussion with the user. Suggest creating a copy of the model to work on. Warn that changes can break reports. Coordinate with the user's CI/CD process. Note when source-level changes are needed outside the semantic model.
- **Success criteria for Tier 1**: Significant performance improvement AND semantic equivalence (identical row count, column count, and data values).
- **Success criteria for Tier 2/3**: Significant performance improvement AND user approval of the changed output or model structure.

## Relationship to Other Skills

- **powerbi-semantic-model**: Use for creating/editing measures and model structure. If the user asks to *optimize* a measure's performance, defer here. If they ask to *create* or *edit* a measure, use `powerbi-semantic-model`.
- **powerbi-tmdl**: Use for TMDL file syntax. Not relevant to performance tuning.
- **powerbi-pbir**: Use for report authoring. Not relevant to performance tuning.
- **fabric-cli**: Use for discovering workspaces and semantic models. Can help locate the dataset to connect to before starting optimization.

---

## Phase 1: Establish Baseline

### Step 1: Resolve All Measure and Function Definitions

Before optimizing, you need full visibility into every DAX expression being evaluated. The user's query references measures that may themselves reference other measures, creating a dependency chain you must fully resolve.

1. **Identify measure references** in the user's query. Look for `[MeasureName]` patterns — these are implicit measure calls.
2. **Retrieve each measure's expression** using `measure_operations` → Get operation, specifying the measure name and its table.
3. **Check for nested dependencies**: Read each measure's expression and identify any `[OtherMeasure]` references within it. Recursively retrieve those definitions too.
4. **Retrieve user-defined function definitions** if the query or any measure references them. Use `function_operations` → Get or List.
5. **Build a DEFINE block** that explicitly defines ALL resolved measures and functions. This "inlines" the definitions so you can see and optimize them directly.

**Example**: If the user's query contains `[Profit Margin]`, and that measure's expression is `DIVIDE([Total Profit], [Total Revenue])`, you need to also retrieve `[Total Profit]` and `[Total Revenue]` definitions, then build:

```dax
DEFINE
    MEASURE Sales[Total Revenue] = SUM(Sales[Revenue])
    MEASURE Sales[Total Profit] = SUM(Sales[Revenue]) - SUM(Sales[Cost])
    MEASURE Sales[Profit Margin] = DIVIDE([Total Profit], [Total Revenue])

EVALUATE
SUMMARIZECOLUMNS(
    Product[Category],
    "Profit Margin", [Profit Margin]
)
```

### Step 2: Gather Model Context

Collect relevant model metadata to inform optimization decisions:

1. Use `table_operations` → List to understand the model's table structure.
2. Use `relationship_operations` → List to understand how tables are joined — this affects filter propagation.

This context helps you identify whether issues come from model design (e.g., missing star schema, bi-directional relationships) versus DAX expression logic.

### Step 3: Execute Baseline (Multi-Run)

Run the enhanced query (with DEFINE block) multiple times to get a reliable baseline, taking the fastest cold-cache run.

**For each run (repeat 3 times):**

1. **Clear the cache** using `dax_query_operations` → ClearCache. This ensures cold-cache execution.
2. **Execute the query** using `dax_query_operations` → Execute with `GetExecutionMetrics` set to `true`. This automatically:
   - Creates/resumes a trace with required events (QueryBegin/End, VertiPaqSEQueryBegin/End, DirectQueryBegin/End, ExecutionMetrics, Error)
   - Clears any previously captured trace events
   - Executes the query
   - Returns `CalculatedExecutionMetrics` (TotalDuration, FormulaEngineDuration, StorageEngineDuration, StorageEngineQueryCount, StorageEngineCpuTime, VertipaqCacheMatches) and `ReportedExecutionMetrics` (server-reported JSON including Direct Lake diagnostics).
3. **Immediately fetch detailed trace events** using `trace_operations` → Fetch. This returns the raw Storage Engine events with xmSQL text, per-query duration, and CpuTime. Estimated rows and KB are embedded in TextData as `[Estimated size (volume, marshalling bytes): X, Y]`. **You must fetch events after each execution because the next Execute call will clear them.**
4. **Record** the TotalDuration and all metrics for this run.

**After all runs:**
- Select the **fastest run** (lowest TotalDuration) as the baseline.
- Record its full metrics and trace events for analysis.

### Step 4: Analyze Baseline

Use **Section 2: Reading and Diagnosing Traces** in `references/dax-performance-reference.md` to interpret the baseline metrics and trace events. Cross-reference findings with **Section 3: Tier 1–2 Query Optimization** to identify which patterns are present.

---

## Phase 2: Optimization Iterations

### Step 1: Identify Optimization Opportunities

Based on your baseline analysis, consult **Section 3: Tier 1–2 Query Optimization** in `references/dax-performance-reference.md` to identify which optimizations to apply.

### Step 2: Modify Measure Definitions

**CRITICAL**: Only modify the measure definitions in the DEFINE block. Do NOT change the EVALUATE clause or SUMMARIZECOLUMNS grouping columns. The query structure must remain identical to preserve semantic equivalence.

Apply one or more optimizations to the DEFINE block measures. For example:

```dax
-- BASELINE measure
DEFINE
    MEASURE Sales[HighValueCount] =
        SUMX(Sales, IF(Sales[Amount] > 1000, 1, 0))

-- OPTIMIZED measure (DAX006: IF→INT, DAX009: FILTER→CALCULATETABLE)
DEFINE
    MEASURE Sales[HighValueCount] =
        CALCULATETABLE(COUNTROWS(Sales), Sales[Amount] > 1000)
```

### Step 3: Execute and Compare

1. **Clear cache** → `dax_query_operations` ClearCache.
2. **Execute optimized query** → `dax_query_operations` Execute with `GetExecutionMetrics=true`.
3. **Fetch trace events** → `trace_operations` Fetch.
4. **Run 3 times** (same multi-run approach as baseline), take the fastest.

**Compare to baseline:**
- **Calculate improvement**: `(BaselineDuration - OptimizedDuration) / BaselineDuration * 100`
- **Verify semantic equivalence**: The optimized query must return the same number of rows, the same columns, and the same data values as the baseline. If results differ, the optimization changed calculation semantics — revert it.

### Step 4: Iterate

- If improvement is **>= 10%** and results are semantically equivalent → **Success**. Present the optimized query and the improvement to the user.
- If improvement is **< 10%** → Try a different optimization strategy. Revisit the trace analysis for other bottlenecks.
- If results **differ** → The optimization changed semantics. Revert and try a different approach.
- If **multiple opportunities** exist, apply incrementally. Test each change individually to understand its impact, then combine the successful ones.

After achieving a successful optimization, **offer to continue**: the optimized query can become the new baseline for further optimization rounds. This iterative approach can achieve compound improvements.

---

## Phase 3: Query Structure Patterns (Tier 2)

If Tier 1 optimizations have been exhausted or the bottleneck is inherent to the query grain, consult **Section 3: Tier 1–2 Query Optimization** (QRY001–003) in `references/dax-performance-reference.md`.

**Before making any changes:**
1. Explain the specific recommendation (e.g., "Aggregating by month instead of day would reduce the result from 365K rows to 12K rows").
2. Explain the trade-off — what the user gains in performance vs. what changes in the output.
3. Wait for explicit user approval.
4. If approved, modify the query structure, re-run the baseline/optimization cycle, and present the results.

---

## Phase 4: Model Optimization Recommendations (Tier 3)

If the bottleneck is in the data model itself (e.g., missing star schema, high-cardinality string columns, missing aggregation tables, problematic relationships), consult **Section 4: Tier 3–4 Model and Data Layout** in `references/dax-performance-reference.md`.

**This is the highest-risk tier. Before proceeding:**
1. Present the diagnosis and specific model change recommendations.
2. Explain why the model design is causing the performance issue.
3. Warn the user that model changes can break downstream reports and visuals.
4. Suggest creating a copy of the semantic model to experiment on.
5. Identify if changes require upstream work (Lakehouse tables, Warehouse views, Power Query transformations) — these cannot be done through the modeling MCP alone.
6. If the user agrees, coordinate with their CI/CD process. Use other skills as needed (`powerbi-semantic-model` for model structure, `fabric-cli` for workspace operations).
7. After applying changes, re-run the full optimization workflow to measure the impact.

---

## Error Handling

- **Connection failure**: Verify the dataset name, workspace name, or XMLA endpoint. For desktop, ensure Power BI Desktop is running. For service, ensure XMLA read/write is enabled on the capacity.
- **Query syntax error**: Use `dax_query_operations` Validate to check DAX syntax before executing.
- **Semantic equivalence failure**: The optimized query returns different results. Review the measure logic — the optimization changed calculation semantics, not just performance. Common causes: changed filter context, modified aggregation granularity, altered CALCULATE filter arguments.
- **No improvement found**: Some queries are already well-optimized. Consider whether the bottleneck is in the data model itself (see Phase 4: Model Optimization Recommendations) or the query structure (see Phase 3: Query Structure Patterns).
- **Trace events empty**: Ensure `GetExecutionMetrics=true` was set on the Execute call. The trace is automatically managed when this flag is set.
